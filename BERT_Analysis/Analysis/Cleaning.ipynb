{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import re\n",
    "import csv\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# change this directory to a folder with all the files you want in it \n",
    "# directory_path = '/home/seh6fy/git/dspg22ari2/BERT_Analysis/GAT'\n",
    "#directory_path = '/home/seh6fy/git/dspg22ari2/BERT_Analysis/Temporary'\n",
    "#directory_path = '/home/seh6fy/git/dspg22ari2/BERT_Analysis/FM_Docs'\n",
    "directory_path = '/home/jhj5dh/ARI/dspg22ari2/BERT_Analysis/GAT'\n",
    "#directory_path = '/sfs/qumulo/qhome/jhj5dh/ARI/dspg22ari2/BERT_Analysis/Analysis'\n",
    "# list of file names in the directory\n",
    "list_of_files = os.listdir(directory_path)\n",
    "\n",
    "# initialize an empty dataframe to store the text documents\n",
    "df = pd.DataFrame(columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "# displays the number of words in each document \n",
    "# update the path to be a folder with the files you want to analyze in it \n",
    "folder_path = '/home/jhj5dh/ARI/dspg22ari2/BERT_Analysis/GAT'\n",
    "#folder_path = \"/home/seh6fy/git/dspg22ari2/BERT_Analysis/FM_Docs\"\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "#directory = \"/home/seh6fy/git/dspg22ari2/BERT_Analysis/FM_Docs\"  # replace with the directory path\n",
    "directory = '/home/jhj5dh/ARI/dspg22ari2/BERT_Analysis/GAT'\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "top_n = 10  # number of top words to display\n",
    "\n",
    "def plot_top_words(file_path):\n",
    "    # read the contents of the file\n",
    "    with open(file_path, \"r\") as f:\n",
    "        contents = f.read()\n",
    "\n",
    "    # split the contents into words\n",
    "    words = contents.split()\n",
    "     #remove stop words\n",
    "    stopwords = ['the', 'and', 'to', 'of', 'is', 'a', 'in', '.', 'or', 'for']\n",
    "    words = [word for word in contents.split() if word.lower() not in stopwords]\n",
    "\n",
    "\n",
    "    # initialize a Counter object to keep track of word frequencies\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    # get the top n most common words\n",
    "    top_words = word_counts.most_common(top_n)\n",
    "\n",
    "    # extract the words and frequencies as separate lists\n",
    "    words, frequencies = zip(*top_words)\n",
    "\n",
    "    # create a bar graph of the top words\n",
    "    plt.bar(words, frequencies)\n",
    "    plt.title(\"Top {} words in {}\".format(top_n, os.path.basename(file_path)))\n",
    "    plt.xlabel(\"Word\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xticks(fontsize=8)\n",
    "    plt.show()\n",
    "\n",
    "# loop through all files in the directory and plot the top words for each file\n",
    "for file_path in glob.glob(os.path.join(directory, \"*\")):\n",
    "    plot_top_words(file_path)\n",
    "\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "#folder_path = \"/home/seh6fy/git/dspg22ari2/BERT_Analysis/FM_Docs\"\n",
    "folder_path = '/home/jhj5dh/ARI/dspg22ari2/BERT_Analysis/GAT'\n",
    "# Get a list of all the files in the folder\n",
    "file_list = glob.glob(os.path.join(folder_path, '*.txt'))\n",
    "\n",
    "# Iterate through each file and count the words\n",
    "for file_path in file_list:\n",
    "    with open(file_path, 'r') as file:\n",
    "        words = file.read().split()\n",
    "        word_count = len(words)\n",
    "        \n",
    "        # Create an empty dictionary to store word counts\n",
    "        word_counts = {}\n",
    "        \n",
    "        # Iterate through each word and update the dictionary\n",
    "        for word in words:\n",
    "            if word in word_counts:\n",
    "                word_counts[word] += 1\n",
    "            else:\n",
    "                word_counts[word] = 1\n",
    "                \n",
    "        # Print the word counts for the file\n",
    "        #print(f\"{file_path}: {word_count} words\")\n",
    "        #for word, count in word_counts.items():\n",
    "            #print(f\"    {word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for filename in list_of_files:\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    # check if the path is a file\n",
    "    if os.path.isfile(file_path):\n",
    "        # read the contents of the file and append as a new row to the dataframe\n",
    "        text = pd.read_csv(file_path, error_bad_lines=False, encoding='utf-8', quoting=csv.QUOTE_NONE, lineterminator='.', header=None)[0].str.cat().strip()\n",
    "        df = df.append({'text': text}, ignore_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line is taking out the new lines so it doesn't display them all as seperate documents\n",
    "df = df.replace(r'\\n',' ', regex=True)\n",
    "# https://medium.com/@armandj.olivares/using-bert-for-classifying-documents-with-long-texts-5c3e7b04573d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace ('\f",
    "', ' ', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import unicodedata\n",
    "df['text'] = df['text'].apply(lambda x: ''.join([' ' if not unicodedata.normalize('NFKD', char).encode('ASCII', 'ignore') else char for char in x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertModel, BertTokenizer# Load the BERT tokenizer\n",
    "from transformers import *\n",
    "# here you could try switching the model from bert-base-cased\n",
    "\n",
    "# # Load the BERT model\n",
    "model = TFAutoModel.from_pretrained(\"bert-base-cased\")\n",
    "# max_seq_length = 512\n",
    "model_name = 'bert-base-uncased'\n",
    "bert_model = TFAutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "from transformers import BertModel, BertTokenizer# Load the BERT tokenizer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# here you could try switching the model from bert-base-cased\n",
    "\n",
    "# # Load the BERT model\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "# max_seq_length = 512\n",
    "model_name = 'bert-base-uncased'\n",
    "bert_model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could look into different encoding methods\n",
    "def encode_document(text, max_tokens=5):\n",
    "    inputs = tokenizer(documents, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "vectors = df['text'].apply(lambda x: encode_document(str(x)))\n",
    "output_vectors = pd.DataFrame(vectors.tolist())\n",
    "\n",
    "# Write the output vectors to a csv file\n",
    "# change the output vector name every time you change the folder\n",
    "output_vectors.to_csv('output_vectors_FM.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could look into different encoding methods\n",
    "def encode_document(text, max_tokens=5):\n",
    "    input_ids = tokenizer.encode(text, max_length=max_tokens, truncation=True, add_special_tokens=True)\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    document_embedding = tf.reduce_mean(last_hidden_state, axis=1)\n",
    "    return document_embedding.numpy().squeeze()\n",
    "\n",
    "vectors = df['text'].apply(lambda x: encode_document(str(x)))\n",
    "output_vectors = pd.DataFrame(vectors.tolist())\n",
    "\n",
    "# Write the output vectors to a csv file\n",
    "# change the output vector name every time you change the folder\n",
    "output_vectors.to_csv('output_vectors_FM.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "doc_names = []\n",
    "# change the directory location to match the one above to have the correct document names\n",
    "for filename in os.listdir('/home/seh6fy/git/dspg22ari2/BERT_Analysis/Docs'):\n",
    "#for filename in os.listdir('/home/seh6fy/git/dspg22ari2/BERT_Analysis/Temporary'):\n",
    "    if filename.endswith('.txt'):\n",
    "        doc_names.append(filename[:-4])\n",
    "# Reduce the dimensionality of the vectors using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "vectors_tsne = tsne.fit_transform(output_vectors)\n",
    "for i, vec in enumerate(vectors_tsne):\n",
    "    ax.scatter(vec[0], vec[1], vec[2], c='b', marker='o', s=5)\n",
    "    ax.text(vec[0], vec[1], vec[2], doc_names[i], color='r', fontsize= 3.5)\n",
    "# Plot the vectors\n",
    "plt.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "doc_names = []\n",
    "# change the directory location to match the one above to have the correct document names\n",
    "for filename in os.listdir('/home/seh6fy/git/dspg22ari2/BERT_Analysis/Docs'):\n",
    "#for filename in os.listdir('/home/seh6fy/git/dspg22ari2/BERT_Analysis/Temporary'):\n",
    "    if filename.endswith('.txt'):\n",
    "        doc_names.append(filename[:-4])\n",
    "\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "vectors_tsne = tsne.fit_transform(output_vectors)\n",
    "\n",
    "# Plot the vectors\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for i, vec in enumerate(vectors_tsne):\n",
    "    ax.scatter(vec[0], vec[1], vec[2], c='b', marker='o', s=5)\n",
    "    ax.text(vec[0], vec[1], vec[2], doc_names[i], color='r', fontsize= 3.5)\n",
    "    \n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Dimension 3')\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "# plt.savefig('plot.pdf')\n",
    "plt.savefig('plot.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "doc_names = []\n",
    "# change the directory location to match the one above to have the correct document names\n",
    "for filename in os.listdir('/home/seh6fy/git/dspg22ari2/BERT_Analysis/Docs'):\n",
    "    if filename.endswith('.txt'):\n",
    "        doc_names.append(filename[:-4])\n",
    "\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "vectors_tsne = tsne.fit_transform(output_vectors)\n",
    "\n",
    "np.savetxt('vectors_tsne.csv', vectors_tsne, delimiter=',')\n",
    "\n",
    "\n",
    "\n",
    "# Plot the vectors\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for i, vec in enumerate(vectors_tsne):\n",
    "    ax.scatter(vec[0], vec[1], vec[2], c='b', marker='o', s=5)\n",
    "    ax.text(vec[0], vec[1], vec[2], doc_names[i], color='r', fontsize= 3.5)\n",
    "\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Dimension 3')\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "# plt.savefig('plot.pdf')\n",
    "plt.savefig('plot.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMPTorch (20201028) Active Learning",
   "language": "python",
   "name": "amptorch-20201028-al"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
