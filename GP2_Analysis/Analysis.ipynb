{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "\n",
    "# change this directory to a folder with all the files you want in it \n",
    "directory_path = '/home/jhj5dh/ARI/dspg22ari2/BERT_Analysis/GAT'\n",
    "\n",
    "# list of file names in the directory\n",
    "list_of_files = os.listdir(directory_path)\n",
    "\n",
    "# initialize an empty dataframe to store the text documents\n",
    "df = pd.DataFrame(columns=['text'])\n",
    "\n",
    "for filename in list_of_files:\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    # check if the path is a file\n",
    "    if os.path.isfile(file_path):\n",
    "        # read the contents of the file and append as a new row to the dataframe\n",
    "        print(file_path)\n",
    "        try:\n",
    "            \n",
    "            text = pd.read_csv(file_path, encoding='utf-8', quoting=csv.QUOTE_NONE, lineterminator='.', header=None)[0].str.cat().strip()\n",
    "        except:\n",
    "            print(filename+\" didn't work\")\n",
    "            \n",
    "        df = pd.concat([df, pd.DataFrame({'text': [text]})], ignore_index=True)\n",
    "\n",
    "\n",
    "# this line is taking out the new lines so it doesn't display them all as separate documents\n",
    "df = df.replace(r'\\n',' ', regex=True)\n",
    "df = df.replace('\f",
    "', ' ', regex=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in list_of_files:\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    # check if the path is a file\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path) as f:\n",
    "            data = f.readlines()\n",
    "            \n",
    "            print(type(data))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "df['text'] = df['text'].apply(lambda x: ''.join([' ' if not unicodedata.normalize('NFKD', char).encode('ASCII', 'ignore') else char for char in x]))\n",
    "\n",
    "model_name = 'gpt2'\n",
    "gp2_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector=tokenizer(\"It is very much like me that I am good at changing myself to adjust to changes in my life It is very much like me that It is difficult for me to adjust to changes It is very much like me that I can usually fit myself into any situation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_document(text):\n",
    "    input_ids = tokenizer.encode(text, max_length=4, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "    outputs = gp2_model(input_ids)\n",
    "    last_hidden_state = outputs.logits\n",
    "    document_embedding = torch.mean(last_hidden_state, dim=1).detach().numpy()\n",
    "    return document_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = df['text'].apply(lambda x: encode_document(str(x)))\n",
    "output_vectors = pd.DataFrame(vectors.tolist())\n",
    "\n",
    "# Write the output vectors to a csv file\n",
    "# change the output vector name every time you change the folder\n",
    "output_vectors.to_csv('output_vectors_FM_GP2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
