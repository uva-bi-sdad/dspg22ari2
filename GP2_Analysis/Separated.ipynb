{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import re\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "import os\n",
    "import fnmatch\n",
    "\n",
    "final = pd.read_csv('vectors_separated_GAT_GP2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# change this directory to a folder with all the files you want in it \n",
    "directory_path = '/home/seh6fy/git/dspg22ari2/BERT_Analysis/GAT'\n",
    "\n",
    "# list of file names in the directory\n",
    "list_of_files = os.listdir(directory_path)\n",
    "\n",
    "# initialize an empty dataframe to store the text documents\n",
    "df = pd.DataFrame(columns=['text', 'category'])\n",
    "\n",
    "for filename in list_of_files:\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    # check if the path is a file\n",
    "    if os.path.isfile(file_path):\n",
    "        # read the contents of the file and append as a new row to the dataframe\n",
    "        text = pd.read_csv(file_path, on_bad_lines='skip', encoding='utf-8', quoting=csv.QUOTE_NONE, lineterminator='.', header=None)[0].str.cat().strip()\n",
    "        if fnmatch.fnmatch(file_path, '*good*'):\n",
    "                category = 'G'\n",
    "        if fnmatch.fnmatch(file_path, '*bad*'):\n",
    "                category = 'B'        \n",
    "        df = pd.concat([df, pd.DataFrame({'text': [text], 'category': [category]})], ignore_index=True)\n",
    "        \n",
    "# this line is taking out the new lines so it doesn't display them all as separate documents\n",
    "\n",
    "df = df.replace(r'\\n',' ', regex=True)\n",
    "df = df.replace('\f",
    "', ' ', regex=True)\n",
    "\n",
    "import unicodedata\n",
    "df['text'] = df['text'].apply(lambda x: ''.join([' ' if not unicodedata.normalize('NFKD', char).encode('ASCII', 'ignore') else char for char in x]))\n",
    "\n",
    "model_name = 'gpt2'\n",
    "gp2_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    " \n",
    "# encoding function \n",
    "def encode_document(text, max_tokens=5):\n",
    "    input_ids = tokenizer.encode(text, max_length=max_tokens, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "    vector = gp2_model.transformer.wte.weight[input_ids,:]\n",
    "    document_embedding = torch.flatten(vector, start_dim=0, end_dim=1)\n",
    "    document_embedding = torch.flatten(document_embedding, start_dim=0, end_dim=1).detach().numpy()\n",
    "    return document_embedding\n",
    "\n",
    "# apply to data\n",
    "df['text'] = df['text'].apply(lambda x: encode_document(str(x)))\n",
    "\n",
    "#split up the vectors\n",
    "split_df = pd.DataFrame(df['text'].tolist())\n",
    "\n",
    "#vector dataframe\n",
    "df1 = split_df.iloc[:, : 768]\n",
    "\n",
    "# type dataframe\n",
    "df2 = pd.DataFrame(df['category'])\n",
    "\n",
    "# join\n",
    "final = pd.concat([df1,df2], axis = 1, join = 'inner')\n",
    "\n",
    "#to csv\n",
    "final.to_csv('vectors_separated_GAT_GP2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (6). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (11). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (12). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (13). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (14). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (15). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (17). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (18). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (19). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (20). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (21). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (22). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (23). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (24). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (25). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (26). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (27). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (28). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (29). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (30). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (31). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (32). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (33). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (34). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (35). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n",
      "/tmp/ipykernel_728717/555114875.py:20: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (36). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(scaled_features)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#kmeans for all data\n",
    "\n",
    "# data\n",
    "X = final.iloc[:, 0:768]\n",
    "\n",
    "# scaler function\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# scale the data\n",
    "scaled_features = scaler.fit_transform(X)\n",
    "\n",
    "# initialize vector for silhouette coefficients\n",
    "\n",
    "silhouette_coefficients = []\n",
    "\n",
    "# iterate through possible cluster amounts and apply kmeans\n",
    "\n",
    "for k in range(2, 37):\n",
    "    kmeans = KMeans(init = \"random\", n_clusters = k, n_init = 10, max_iter = 300, random_state = 42)\n",
    "    kmeans.fit(scaled_features)\n",
    "    score = silhouette_score(scaled_features, kmeans.labels_)\n",
    "    silhouette_coefficients.append(score)\n",
    "    \n",
    "# plot silhouette coefficients\n",
    "plt.plot(range(2,37), silhouette_coefficients)\n",
    "\n",
    "#ideal is 5...calculate for this and add to \"final\"\n",
    "\n",
    "kmeans = KMeans(init = \"random\", n_clusters = 5, n_init = 10, max_iter = 300, random_state = 42)\n",
    "kmeans.fit(scaled_features)\n",
    "\n",
    "final.loc[:,'cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay but 5 doesnt make sense let's try 2\n",
    "\n",
    "kmeans = KMeans(init = \"random\", n_clusters = 2, n_init = 10, max_iter = 300, random_state = 42)\n",
    "kmeans.fit(scaled_features)\n",
    "final.loc[:,'cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"good\" half of the data\n",
    "good_df = final[final['category'] == 'G']\n",
    "good = good_df.iloc[:, 0:768]\n",
    "\n",
    "good_scaled = scaler.fit_transform(good)\n",
    "\n",
    "good_silhouette_coefficients = []\n",
    "\n",
    "#same for loop\n",
    "for k in range(2, 18):\n",
    "    kmeans = KMeans(init = \"random\", n_clusters = k, n_init = 10, max_iter = 300, random_state = 42)\n",
    "    kmeans.fit(good_scaled)\n",
    "    score = silhouette_score(good_scaled, kmeans.labels_)\n",
    "    good_silhouette_coefficients.append(score)\n",
    "    \n",
    "plt.plot(range(2,18), good_silhouette_coefficients)\n",
    "\n",
    "# ideal is 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"bad\" data\n",
    "bad_df = final[final['category'] == 'B']\n",
    "bad = bad_df.iloc[:, 0:768]\n",
    "\n",
    "bad_scaled = scaler.fit_transform(bad)\n",
    "\n",
    "bad_silhouette_coefficients = []\n",
    "\n",
    "# same for loop\n",
    "for k in range(2, 19):\n",
    "    kmeans = KMeans(init = \"random\", n_clusters = k, n_init = 10, max_iter = 300, random_state = 42)\n",
    "    kmeans.fit(bad_scaled)\n",
    "    score = silhouette_score(bad_scaled, kmeans.labels_)\n",
    "    bad_silhouette_coefficients.append(score)\n",
    "    \n",
    "plt.plot(range(2,19), bad_silhouette_coefficients)\n",
    "\n",
    "#ideal is 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pca as pca\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)\n",
    "principalComponents = pca.fit_transform(scaled_features)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
    "finalDf = pd.concat([principalDf, final['cluster']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = finalDf['principal component 1']\n",
    "y = finalDf['principal component 2']\n",
    "plt.scatter(x, y, c=finalDf['cluster'], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>...</th>\n",
       "      <th>743</th>\n",
       "      <th>744</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "      <th>754</th>\n",
       "      <th>755</th>\n",
       "      <th>756</th>\n",
       "      <th>757</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.039030</td>\n",
       "      <td>-0.086941</td>\n",
       "      <td>0.066183</td>\n",
       "      <td>-0.053097</td>\n",
       "      <td>-0.087962</td>\n",
       "      <td>-0.068795</td>\n",
       "      <td>-0.217095</td>\n",
       "      <td>-0.041003</td>\n",
       "      <td>0.047356</td>\n",
       "      <td>-0.002874</td>\n",
       "      <td>-0.010789</td>\n",
       "      <td>0.001914</td>\n",
       "      <td>-0.035281</td>\n",
       "      <td>0.065953</td>\n",
       "      <td>0.032943</td>\n",
       "      <td>0.025592</td>\n",
       "      <td>0.064494</td>\n",
       "      <td>0.058007</td>\n",
       "      <td>-0.021952</td>\n",
       "      <td>0.156676</td>\n",
       "      <td>-0.085778</td>\n",
       "      <td>0.033980</td>\n",
       "      <td>-0.108128</td>\n",
       "      <td>-0.009755</td>\n",
       "      <td>0.042683</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059958</td>\n",
       "      <td>0.026460</td>\n",
       "      <td>0.198716</td>\n",
       "      <td>0.099251</td>\n",
       "      <td>-0.024556</td>\n",
       "      <td>-0.097274</td>\n",
       "      <td>-0.057142</td>\n",
       "      <td>0.018027</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>0.138355</td>\n",
       "      <td>0.101647</td>\n",
       "      <td>-0.037653</td>\n",
       "      <td>-0.037585</td>\n",
       "      <td>-0.089195</td>\n",
       "      <td>-0.013962</td>\n",
       "      <td>-0.054625</td>\n",
       "      <td>-0.014361</td>\n",
       "      <td>-0.063574</td>\n",
       "      <td>-0.019431</td>\n",
       "      <td>-0.174063</td>\n",
       "      <td>-0.002174</td>\n",
       "      <td>0.002537</td>\n",
       "      <td>0.121516</td>\n",
       "      <td>-0.125454</td>\n",
       "      <td>0.053616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.009720</td>\n",
       "      <td>-0.081880</td>\n",
       "      <td>0.106781</td>\n",
       "      <td>0.041525</td>\n",
       "      <td>-0.123429</td>\n",
       "      <td>-0.043841</td>\n",
       "      <td>-0.217286</td>\n",
       "      <td>-0.097202</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.064409</td>\n",
       "      <td>0.017289</td>\n",
       "      <td>0.042813</td>\n",
       "      <td>0.017757</td>\n",
       "      <td>-0.031487</td>\n",
       "      <td>0.137280</td>\n",
       "      <td>-0.023062</td>\n",
       "      <td>0.008646</td>\n",
       "      <td>0.079860</td>\n",
       "      <td>-0.060788</td>\n",
       "      <td>0.173512</td>\n",
       "      <td>-0.148557</td>\n",
       "      <td>0.040863</td>\n",
       "      <td>0.001970</td>\n",
       "      <td>-0.034032</td>\n",
       "      <td>-0.010577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054208</td>\n",
       "      <td>0.022216</td>\n",
       "      <td>0.187954</td>\n",
       "      <td>0.212609</td>\n",
       "      <td>0.029206</td>\n",
       "      <td>-0.107148</td>\n",
       "      <td>-0.016082</td>\n",
       "      <td>-0.000909</td>\n",
       "      <td>-0.007651</td>\n",
       "      <td>-0.109760</td>\n",
       "      <td>0.010323</td>\n",
       "      <td>-0.025526</td>\n",
       "      <td>0.006802</td>\n",
       "      <td>-0.061828</td>\n",
       "      <td>-0.032853</td>\n",
       "      <td>-0.155281</td>\n",
       "      <td>0.004749</td>\n",
       "      <td>0.086047</td>\n",
       "      <td>0.057976</td>\n",
       "      <td>-0.202903</td>\n",
       "      <td>-0.044898</td>\n",
       "      <td>-0.021528</td>\n",
       "      <td>0.242567</td>\n",
       "      <td>-0.065737</td>\n",
       "      <td>-0.062649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.147399</td>\n",
       "      <td>-0.095851</td>\n",
       "      <td>0.142954</td>\n",
       "      <td>-0.106241</td>\n",
       "      <td>-0.012254</td>\n",
       "      <td>-0.095917</td>\n",
       "      <td>-0.217286</td>\n",
       "      <td>-0.113740</td>\n",
       "      <td>0.091689</td>\n",
       "      <td>0.080804</td>\n",
       "      <td>-0.083746</td>\n",
       "      <td>0.106492</td>\n",
       "      <td>-0.038799</td>\n",
       "      <td>0.113077</td>\n",
       "      <td>0.004294</td>\n",
       "      <td>-0.178117</td>\n",
       "      <td>0.128131</td>\n",
       "      <td>0.064309</td>\n",
       "      <td>-0.011330</td>\n",
       "      <td>0.123660</td>\n",
       "      <td>-0.023536</td>\n",
       "      <td>0.110805</td>\n",
       "      <td>-0.090541</td>\n",
       "      <td>-0.052150</td>\n",
       "      <td>0.043946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050900</td>\n",
       "      <td>0.004761</td>\n",
       "      <td>0.161291</td>\n",
       "      <td>0.066715</td>\n",
       "      <td>-0.042453</td>\n",
       "      <td>-0.001744</td>\n",
       "      <td>0.065781</td>\n",
       "      <td>0.035206</td>\n",
       "      <td>-0.038102</td>\n",
       "      <td>0.046104</td>\n",
       "      <td>0.059788</td>\n",
       "      <td>-0.039233</td>\n",
       "      <td>-0.088627</td>\n",
       "      <td>-0.158248</td>\n",
       "      <td>-0.099372</td>\n",
       "      <td>0.043514</td>\n",
       "      <td>0.074937</td>\n",
       "      <td>-0.038973</td>\n",
       "      <td>-0.058922</td>\n",
       "      <td>-0.154869</td>\n",
       "      <td>0.078405</td>\n",
       "      <td>-0.042030</td>\n",
       "      <td>0.102986</td>\n",
       "      <td>-0.062511</td>\n",
       "      <td>-0.113120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.025272</td>\n",
       "      <td>-0.081084</td>\n",
       "      <td>0.193862</td>\n",
       "      <td>0.160525</td>\n",
       "      <td>-0.008502</td>\n",
       "      <td>-0.096821</td>\n",
       "      <td>-0.221240</td>\n",
       "      <td>-0.008060</td>\n",
       "      <td>-0.131907</td>\n",
       "      <td>-0.143298</td>\n",
       "      <td>-0.169097</td>\n",
       "      <td>0.290681</td>\n",
       "      <td>-0.072676</td>\n",
       "      <td>-0.322109</td>\n",
       "      <td>0.030075</td>\n",
       "      <td>-0.053443</td>\n",
       "      <td>0.066640</td>\n",
       "      <td>0.178125</td>\n",
       "      <td>-0.007738</td>\n",
       "      <td>0.225445</td>\n",
       "      <td>0.077549</td>\n",
       "      <td>-0.059251</td>\n",
       "      <td>0.261462</td>\n",
       "      <td>-0.062177</td>\n",
       "      <td>0.018564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059017</td>\n",
       "      <td>0.231748</td>\n",
       "      <td>0.144958</td>\n",
       "      <td>0.168369</td>\n",
       "      <td>-0.201428</td>\n",
       "      <td>-0.096312</td>\n",
       "      <td>-0.190247</td>\n",
       "      <td>0.014375</td>\n",
       "      <td>0.085794</td>\n",
       "      <td>0.088493</td>\n",
       "      <td>-0.249973</td>\n",
       "      <td>0.055531</td>\n",
       "      <td>-0.162203</td>\n",
       "      <td>-0.171385</td>\n",
       "      <td>0.071432</td>\n",
       "      <td>0.081647</td>\n",
       "      <td>-0.176273</td>\n",
       "      <td>-0.049244</td>\n",
       "      <td>-0.000223</td>\n",
       "      <td>-0.186317</td>\n",
       "      <td>-0.198262</td>\n",
       "      <td>-0.097490</td>\n",
       "      <td>0.053789</td>\n",
       "      <td>0.142623</td>\n",
       "      <td>-0.145674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.037293</td>\n",
       "      <td>0.011009</td>\n",
       "      <td>0.055412</td>\n",
       "      <td>-0.066392</td>\n",
       "      <td>-0.073041</td>\n",
       "      <td>-0.039873</td>\n",
       "      <td>-0.217285</td>\n",
       "      <td>-0.001084</td>\n",
       "      <td>0.049005</td>\n",
       "      <td>-0.063712</td>\n",
       "      <td>0.135720</td>\n",
       "      <td>0.049350</td>\n",
       "      <td>0.041995</td>\n",
       "      <td>-0.039115</td>\n",
       "      <td>0.064111</td>\n",
       "      <td>-0.098084</td>\n",
       "      <td>0.112893</td>\n",
       "      <td>0.077284</td>\n",
       "      <td>-0.014651</td>\n",
       "      <td>0.137914</td>\n",
       "      <td>0.028586</td>\n",
       "      <td>0.018378</td>\n",
       "      <td>-0.111337</td>\n",
       "      <td>0.038305</td>\n",
       "      <td>0.026590</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077039</td>\n",
       "      <td>-0.080395</td>\n",
       "      <td>0.211426</td>\n",
       "      <td>0.109116</td>\n",
       "      <td>0.024911</td>\n",
       "      <td>-0.036544</td>\n",
       "      <td>-0.060996</td>\n",
       "      <td>0.039971</td>\n",
       "      <td>-0.005585</td>\n",
       "      <td>0.095389</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>-0.004908</td>\n",
       "      <td>-0.040582</td>\n",
       "      <td>-0.119355</td>\n",
       "      <td>0.017407</td>\n",
       "      <td>-0.003431</td>\n",
       "      <td>0.063120</td>\n",
       "      <td>-0.066817</td>\n",
       "      <td>-0.011499</td>\n",
       "      <td>-0.168559</td>\n",
       "      <td>0.003505</td>\n",
       "      <td>-0.062214</td>\n",
       "      <td>0.105612</td>\n",
       "      <td>-0.187919</td>\n",
       "      <td>0.063430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.039030 -0.086941  0.066183 -0.053097 -0.087962 -0.068795 -0.217095   \n",
       "2 -0.009720 -0.081880  0.106781  0.041525 -0.123429 -0.043841 -0.217286   \n",
       "3  0.147399 -0.095851  0.142954 -0.106241 -0.012254 -0.095917 -0.217286   \n",
       "5 -0.025272 -0.081084  0.193862  0.160525 -0.008502 -0.096821 -0.221240   \n",
       "7 -0.037293  0.011009  0.055412 -0.066392 -0.073041 -0.039873 -0.217285   \n",
       "\n",
       "        7         8         9         10        11        12        13   \\\n",
       "0 -0.041003  0.047356 -0.002874 -0.010789  0.001914 -0.035281  0.065953   \n",
       "2 -0.097202  0.000663  0.064409  0.017289  0.042813  0.017757 -0.031487   \n",
       "3 -0.113740  0.091689  0.080804 -0.083746  0.106492 -0.038799  0.113077   \n",
       "5 -0.008060 -0.131907 -0.143298 -0.169097  0.290681 -0.072676 -0.322109   \n",
       "7 -0.001084  0.049005 -0.063712  0.135720  0.049350  0.041995 -0.039115   \n",
       "\n",
       "        14        15        16        17        18        19        20   \\\n",
       "0  0.032943  0.025592  0.064494  0.058007 -0.021952  0.156676 -0.085778   \n",
       "2  0.137280 -0.023062  0.008646  0.079860 -0.060788  0.173512 -0.148557   \n",
       "3  0.004294 -0.178117  0.128131  0.064309 -0.011330  0.123660 -0.023536   \n",
       "5  0.030075 -0.053443  0.066640  0.178125 -0.007738  0.225445  0.077549   \n",
       "7  0.064111 -0.098084  0.112893  0.077284 -0.014651  0.137914  0.028586   \n",
       "\n",
       "        21        22        23        24   ...       743       744       745  \\\n",
       "0  0.033980 -0.108128 -0.009755  0.042683  ... -0.059958  0.026460  0.198716   \n",
       "2  0.040863  0.001970 -0.034032 -0.010577  ...  0.054208  0.022216  0.187954   \n",
       "3  0.110805 -0.090541 -0.052150  0.043946  ...  0.050900  0.004761  0.161291   \n",
       "5 -0.059251  0.261462 -0.062177  0.018564  ...  0.059017  0.231748  0.144958   \n",
       "7  0.018378 -0.111337  0.038305  0.026590  ... -0.077039 -0.080395  0.211426   \n",
       "\n",
       "        746       747       748       749       750       751       752  \\\n",
       "0  0.099251 -0.024556 -0.097274 -0.057142  0.018027  0.001693  0.138355   \n",
       "2  0.212609  0.029206 -0.107148 -0.016082 -0.000909 -0.007651 -0.109760   \n",
       "3  0.066715 -0.042453 -0.001744  0.065781  0.035206 -0.038102  0.046104   \n",
       "5  0.168369 -0.201428 -0.096312 -0.190247  0.014375  0.085794  0.088493   \n",
       "7  0.109116  0.024911 -0.036544 -0.060996  0.039971 -0.005585  0.095389   \n",
       "\n",
       "        753       754       755       756       757       758       759  \\\n",
       "0  0.101647 -0.037653 -0.037585 -0.089195 -0.013962 -0.054625 -0.014361   \n",
       "2  0.010323 -0.025526  0.006802 -0.061828 -0.032853 -0.155281  0.004749   \n",
       "3  0.059788 -0.039233 -0.088627 -0.158248 -0.099372  0.043514  0.074937   \n",
       "5 -0.249973  0.055531 -0.162203 -0.171385  0.071432  0.081647 -0.176273   \n",
       "7  0.058600 -0.004908 -0.040582 -0.119355  0.017407 -0.003431  0.063120   \n",
       "\n",
       "        760       761       762       763       764       765       766  \\\n",
       "0 -0.063574 -0.019431 -0.174063 -0.002174  0.002537  0.121516 -0.125454   \n",
       "2  0.086047  0.057976 -0.202903 -0.044898 -0.021528  0.242567 -0.065737   \n",
       "3 -0.038973 -0.058922 -0.154869  0.078405 -0.042030  0.102986 -0.062511   \n",
       "5 -0.049244 -0.000223 -0.186317 -0.198262 -0.097490  0.053789  0.142623   \n",
       "7 -0.066817 -0.011499 -0.168559  0.003505 -0.062214  0.105612 -0.187919   \n",
       "\n",
       "        767  \n",
       "0  0.053616  \n",
       "2 -0.062649  \n",
       "3 -0.113120  \n",
       "5 -0.145674  \n",
       "7  0.063430  \n",
       "\n",
       "[5 rows x 768 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.23702163, -0.16967855, -1.0731889 , ..., -0.39847475,\n",
       "        -0.76831764,  1.2091033 ],\n",
       "       [-0.23702163, -0.16967855, -1.0731889 , ..., -0.39847475,\n",
       "        -0.76831764,  1.2091033 ],\n",
       "       [-0.9543767 ,  0.04878046,  0.05148238, ...,  1.6099724 ,\n",
       "         0.3667343 , -0.3467134 ],\n",
       "       ...,\n",
       "       [ 1.3576229 , -0.55425876,  1.0535622 , ..., -0.7059265 ,\n",
       "         0.42805138, -1.0221026 ],\n",
       "       [-0.9543767 ,  0.04878046,  0.05148238, ...,  1.6099724 ,\n",
       "         0.3667343 , -0.3467134 ],\n",
       "       [-0.9543767 ,  0.04878046,  0.05148238, ...,  1.6099724 ,\n",
       "         0.3667343 , -0.3467134 ]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-jns]",
   "language": "python",
   "name": "conda-env-.conda-jns-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
